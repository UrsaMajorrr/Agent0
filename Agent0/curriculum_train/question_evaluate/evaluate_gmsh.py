#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
Description:
    This script evaluates Gmsh meshing tasks generated by the curriculum agent.
    It uses vLLM to generate multiple Gmsh scripts for each task and validates
    that the scripts can be executed successfully.

    Scoring:
    - Scripts are executed in Gmsh to verify they produce valid meshes
    - Majority voting determines the best approach
    - Tasks with consistent, working scripts get higher scores

Setup:
    pip install stopit transformers torch vllm gmsh

Example Usage (in a shell script):
    CUDA_VISIBLE_DEVICES=0 python evaluate_gmsh.py --model "Qwen/Qwen3-4B-Base" --suffix 0 --save_name "gmsh_tasks" &
'''

import json
import vllm
from transformers import AutoTokenizer
import argparse
import re
import os
import tempfile


# --- Argument Parsing ---
parser = argparse.ArgumentParser(description="Evaluate Gmsh tasks using vLLM.")
parser.add_argument("--model", type=str, default="Qwen/Qwen3-4B-Base", help="Path to the model in Hugging Face format.")
parser.add_argument("--num_samples", type=int, default=9, help="Number of candidate scripts to generate per task (n).")
parser.add_argument("--suffix", type=str, default="0", help="A unique suffix for file naming, often the GPU index.")
parser.add_argument("--save_name", type=str, required=True, help="A base name for input and output files.")
args = parser.parse_args()

# --- Constants and Paths ---
STORAGE_PATH = os.getenv("STORAGE_PATH", "")
INPUT_FILE = f"{STORAGE_PATH}/generated_question/{args.save_name}_{args.suffix}.json"
OUTPUT_FILE = f"{STORAGE_PATH}/generated_question/{args.save_name}_{args.suffix}_results.json"

# --- System Prompt for Gmsh Script Generation ---
SYSTEM_PROMPT = """You are an expert in computational meshing using Gmsh.
Write a complete Python script using the Gmsh library to complete the meshing task.
Your script must:
1. Initialize Gmsh with gmsh.initialize()
2. Create the geometry using the Gmsh OCC API
3. Synchronize with gmsh.model.occ.synchronize()
4. Create the physical groups as specified
5. Set mesh sizes
6. Generate the mesh with gmsh.model.mesh.generate(3)
7. Save with gmsh.write("output.msh") and finalize with gmsh.finalize()

Wrap your code in ```python ... ``` blocks."""

# --- Subprocess-based Script Execution (isolated) ---
import subprocess

def execute_gmsh_script(script_code, timeout=30):
    """
    Execute a Gmsh script in subprocess for full isolation.
    Each script runs in its own process with its own gmsh instance.
    Returns tuple (success: bool, num_elements: int, error: str)
    """
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(script_code)
            temp_path = f.name

        # Run in subprocess - complete isolation
        result = subprocess.run(
            ['python', temp_path],
            capture_output=True,
            text=True,
            timeout=timeout
        )

        os.unlink(temp_path)

        # Check output for element count (gmsh prints "X nodes Y elements")
        output = result.stdout + result.stderr
        elem_match = re.search(r'(\d+)\s+nodes\s+(\d+)\s+elements', output)
        if elem_match:
            num_elements = int(elem_match.group(2))
            return (num_elements > 0, num_elements, None)

        # If no match but no error, check return code
        if result.returncode == 0:
            return (True, 1, None)  # Assume success

        return (False, 0, result.stderr[:200] if result.stderr else "Unknown error")

    except subprocess.TimeoutExpired:
        try:
            os.unlink(temp_path)
        except:
            pass
        return (False, 0, "timeout")
    except Exception as e:
        return (False, 0, str(e))


def extract_python_code(text):
    """Extract Python code from markdown code blocks."""
    pattern = r'```python\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return matches[-1].strip()
    # Try without language specifier
    pattern = r'```\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return matches[-1].strip()
    return None


# --- Main Script Logic ---

# 1. Load and Prepare Data
print(f"[{args.suffix}] Loading data from: {INPUT_FILE}")
try:
    with open(INPUT_FILE, "r") as f:
        data = json.load(f)
    # Clean up the input file immediately after loading
    os.remove(INPUT_FILE)
except FileNotFoundError:
    print(f"[{args.suffix}] ERROR: Input file not found. Exiting.")
    exit()

# Filter data - tasks with score >= 0 are valid (score = -1 means invalid format)
valid_data = [item for item in data if item.get('score', -1) >= 0 and item.get('task')]
if not valid_data:
    print(f"[{args.suffix}] No valid tasks to process. Exiting.")
    with open(OUTPUT_FILE, "w") as f:
        json.dump([], f)
    exit()

tasks = [item["task"] for item in valid_data]
print(f"[{args.suffix}] Found {len(tasks)} tasks to process.")

# 2. Initialize Model and Tokenizer
print(f"[{args.suffix}] Initializing vLLM for model: {args.model}")
tokenizer = AutoTokenizer.from_pretrained(args.model)
model = vllm.LLM(
    model=args.model,
    tokenizer=args.model,
    gpu_memory_utilization=0.85,
    seed=int(args.suffix),
)
sample_params = vllm.SamplingParams(
    max_tokens=4096,
    temperature=1.0,
    top_p=1.0,
    top_k=40,
    stop_token_ids=[tokenizer.eos_token_id],
    n=args.num_samples,
)

# 3. Generate Responses
print(f"[{args.suffix}] Generating {args.num_samples} samples for each task...")
chats = [[
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": task}
] for task in tasks]

if tokenizer.chat_template:
    prompts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True, add_special_tokens=True) for chat in chats]
else:
    prompts = ["system: " + chat[0]["content"] + '\n' + "user: " + chat[1]["content"] for chat in chats]

responses = model.generate(prompts, sampling_params=sample_params, use_tqdm=True)
print(f"[{args.suffix}] Generation complete.")

# 4. Process and Validate Responses
results_all = []
print(f"[{args.suffix}] Validating Gmsh scripts...")
for response, task in zip(responses, tasks):
    try:
        # Extract Python code from all generated samples
        scripts = [extract_python_code(output.text) for output in response.outputs]
        scripts = [s for s in scripts if s]  # Filter out None/empty

        if not scripts:
            print(f"[{args.suffix}] WARNING: No valid scripts found for task: '{task[:50]}...'")
            continue

        # Validate each script
        validation_results = []
        for script in scripts:
            result = execute_gmsh_script(script, timeout=30)
            validation_results.append(result)

        # Count successful scripts
        successful = sum(1 for r in validation_results if r[0])
        total = len(validation_results)

        if successful == 0:
            print(f"[{args.suffix}] No working scripts for: '{task[:50]}...'")
            continue

        # Score based on success rate
        score = successful / total

        # Find the best script (most elements generated)
        best_idx = max(range(len(validation_results)),
                       key=lambda i: validation_results[i][1] if validation_results[i][0] else -1)
        best_script = scripts[best_idx]

        results_all.append({
            "question": task,  # Keep as 'question' for compatibility
            "answer": best_script,  # The best working script
            "score": score,
            "successful_scripts": successful,
            "total_scripts": total,
        })

    except Exception as e:
        print(f"[{args.suffix}] CRITICAL ERROR processing task '{task[:50]}...': {e}")
        continue

# 5. Save Final Results
print(f"[{args.suffix}] Processed {len(results_all)} tasks. Saving results to: {OUTPUT_FILE}")
with open(OUTPUT_FILE, "w") as f:
    json.dump(results_all, f, indent=4)

print(f"[{args.suffix}] Script finished.")
