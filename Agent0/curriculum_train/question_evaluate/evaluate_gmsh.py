#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
Description:
    This script evaluates Gmsh meshing tasks generated by the curriculum agent.
    It uses vLLM to generate multiple Gmsh scripts for each task and validates
    that the scripts can be executed successfully.

    Scoring:
    - Scripts are executed in Gmsh to verify they produce valid meshes
    - Majority voting determines the best approach
    - Tasks with consistent, working scripts get higher scores

Setup:
    pip install stopit transformers torch vllm gmsh

Example Usage (in a shell script):
    CUDA_VISIBLE_DEVICES=0 python evaluate_gmsh.py --model "Qwen/Qwen3-4B-Base" --suffix 0 --save_name "gmsh_tasks" &
'''

import json
import vllm
from transformers import AutoTokenizer
import argparse
import re
import os
import tempfile


# --- Argument Parsing ---
parser = argparse.ArgumentParser(description="Evaluate Gmsh tasks using vLLM.")
parser.add_argument("--model", type=str, default="Qwen/Qwen3-4B-Base", help="Path to the model in Hugging Face format.")
parser.add_argument("--num_samples", type=int, default=9, help="Number of candidate scripts to generate per task (n).")
parser.add_argument("--suffix", type=str, default="0", help="A unique suffix for file naming, often the GPU index.")
parser.add_argument("--save_name", type=str, required=True, help="A base name for input and output files.")
args = parser.parse_args()

# --- Constants and Paths ---
STORAGE_PATH = os.getenv("STORAGE_PATH", "")
INPUT_FILE = f"{STORAGE_PATH}/generated_question/{args.save_name}_{args.suffix}.json"
OUTPUT_FILE = f"{STORAGE_PATH}/generated_question/{args.save_name}_{args.suffix}_results.json"

# --- System Prompt for Gmsh Script Generation ---
GMSH_SYSTEM_PROMPT = """You are an expert in computational meshing using Gmsh.
Write a complete Python script using the Gmsh library to complete the meshing task.

IMPORTANT: The task specifies a geometry file path (e.g., "geometries/xxxxx.step"). You MUST use that EXACT path in your gmsh.model.occ.importShapes() call.

Your script must:
1. Load the STEP file using the EXACT path from the task with gmsh.model.occ.importShapes()
2. Create the physical groups as specified in the task
3. Apply the mesh refinement settings as specified
4. Generate a quality 3D mesh

## GMSH API Reference

### Initialization and Loading STEP Files
```python
import gmsh
gmsh.initialize()
gmsh.model.add("model_name")

# Import CAD geometry from STEP file
entities = gmsh.model.occ.importShapes("path/to/geometry.step")
gmsh.model.occ.synchronize()

# Get all volumes and surfaces after import
volumes = gmsh.model.getEntities(dim=3)  # [(3, vol_tag), ...]
surfaces = gmsh.model.getEntities(dim=2)  # [(2, surf_tag), ...]
```

### Physical Groups (required for FEA)
```python
# Get entities by dimension: 0=points, 1=curves, 2=surfaces, 3=volumes
volumes = gmsh.model.getEntities(dim=3)
surfaces = gmsh.model.getEntities(dim=2)

# Create physical groups with meaningful names
vol_group = gmsh.model.addPhysicalGroup(3, [v[1] for v in volumes])
gmsh.model.setPhysicalName(3, vol_group, "domain")

# Group specific surfaces by their tags
surf_group = gmsh.model.addPhysicalGroup(2, [surf_tag])
gmsh.model.setPhysicalName(2, surf_group, "boundary_name")
```

### Mesh Size Fields (Local Refinement)
```python
# Distance field - refine near surfaces
gmsh.model.mesh.field.add("Distance", 1)
gmsh.model.mesh.field.setNumbers(1, "SurfacesList", [surf_tag1, surf_tag2])

# Threshold field - size based on distance
gmsh.model.mesh.field.add("Threshold", 2)
gmsh.model.mesh.field.setNumber(2, "InField", 1)
gmsh.model.mesh.field.setNumber(2, "SizeMin", 0.1)   # size at surface
gmsh.model.mesh.field.setNumber(2, "SizeMax", 1.0)   # size far away
gmsh.model.mesh.field.setNumber(2, "DistMin", 0.5)   # distance for SizeMin
gmsh.model.mesh.field.setNumber(2, "DistMax", 5.0)   # distance for SizeMax

# Box field - refine in region
gmsh.model.mesh.field.add("Box", 3)
gmsh.model.mesh.field.setNumber(3, "VIn", 0.2)   # size inside box
gmsh.model.mesh.field.setNumber(3, "VOut", 1.0)  # size outside
gmsh.model.mesh.field.setNumber(3, "XMin", -1)
gmsh.model.mesh.field.setNumber(3, "XMax", 1)
gmsh.model.mesh.field.setNumber(3, "YMin", -1)
gmsh.model.mesh.field.setNumber(3, "YMax", 1)
gmsh.model.mesh.field.setNumber(3, "ZMin", -1)
gmsh.model.mesh.field.setNumber(3, "ZMax", 1)

# Combine fields with Min
gmsh.model.mesh.field.add("Min", 4)
gmsh.model.mesh.field.setNumbers(4, "FieldsList", [2, 3])
gmsh.model.mesh.field.setAsBackgroundMesh(4)
```

### Transfinite Meshing (Structured Meshes)
```python
# Set number of nodes on curves
gmsh.model.mesh.setTransfiniteCurve(curve_tag, numNodes, meshType="Progression", coef=1.0)

# Set transfinite surface (requires 3 or 4 corner points)
gmsh.model.mesh.setTransfiniteSurface(surf_tag, arrangement="Left", cornerTags=[p1, p2, p3, p4])

# Recombine triangles into quads
gmsh.model.mesh.setRecombine(2, surf_tag)
```

### Boundary Layer Meshing
```python
gmsh.model.mesh.field.add("BoundaryLayer", 1)
gmsh.model.mesh.field.setNumbers(1, "SurfacesList", [surf1, surf2])
gmsh.model.mesh.field.setNumber(1, "Size", 0.01)       # first layer thickness
gmsh.model.mesh.field.setNumber(1, "Ratio", 1.2)       # growth ratio
gmsh.model.mesh.field.setNumber(1, "NbLayers", 10)     # number of layers
gmsh.model.mesh.field.setNumber(1, "Quads", 1)         # use quads
gmsh.model.mesh.field.setAsBoundaryLayer(1)
```

### Mesh Options
```python
# Global mesh size
gmsh.option.setNumber("Mesh.CharacteristicLengthMax", 2.0)
gmsh.option.setNumber("Mesh.CharacteristicLengthMin", 0.1)

# Mesh from curvature (elements per 2*pi radians)
gmsh.option.setNumber("Mesh.MeshSizeFromCurvature", 20)

# Algorithm selection
gmsh.option.setNumber("Mesh.Algorithm", 6)      # 2D: Frontal-Delaunay
gmsh.option.setNumber("Mesh.Algorithm3D", 10)   # 3D: HXT (fast)

# Quality optimization
gmsh.option.setNumber("Mesh.OptimizeNetgen", 1)
gmsh.option.setNumber("Mesh.Smoothing", 10)

# Element order
gmsh.option.setNumber("Mesh.ElementOrder", 1)   # 1=linear, 2=quadratic
```

### Finalization
```python
gmsh.model.mesh.generate(3)  # Generate 3D mesh
gmsh.write("output.msh")
gmsh.finalize()
```

## Example: Loading STEP file with refinement near cylindrical surfaces
```python
import gmsh
gmsh.initialize()
gmsh.model.add("thermal_mesh")

# Load the STEP geometry
gmsh.model.occ.importShapes("geometries/example_part.step")
gmsh.model.occ.synchronize()

# Get all entities
volumes = gmsh.model.getEntities(dim=3)
surfaces = gmsh.model.getEntities(dim=2)

# Create physical groups
vol_group = gmsh.model.addPhysicalGroup(3, [v[1] for v in volumes])
gmsh.model.setPhysicalName(3, vol_group, "domain")

# Group all surfaces as boundary (or split by surface type if needed)
all_surf_tags = [s[1] for s in surfaces]
surf_group = gmsh.model.addPhysicalGroup(2, all_surf_tags)
gmsh.model.setPhysicalName(2, surf_group, "boundary")

# Distance field for refinement near specific surfaces
gmsh.model.mesh.field.add("Distance", 1)
gmsh.model.mesh.field.setNumbers(1, "SurfacesList", all_surf_tags[:3])  # refine near first 3 surfaces

# Threshold field
gmsh.model.mesh.field.add("Threshold", 2)
gmsh.model.mesh.field.setNumber(2, "InField", 1)
gmsh.model.mesh.field.setNumber(2, "SizeMin", 0.5)
gmsh.model.mesh.field.setNumber(2, "SizeMax", 2.0)
gmsh.model.mesh.field.setNumber(2, "DistMin", 0.5)
gmsh.model.mesh.field.setNumber(2, "DistMax", 5.0)
gmsh.model.mesh.field.setAsBackgroundMesh(2)

# Global mesh settings
gmsh.option.setNumber("Mesh.Algorithm3D", 10)
gmsh.option.setNumber("Mesh.OptimizeNetgen", 1)

# Generate mesh
gmsh.model.mesh.generate(3)
gmsh.write("output.msh")
gmsh.finalize()
```

Wrap your code in ```python ... ``` blocks."""

# ---------------------------- GPU Idle Worker ------------------- #
stop_event = threading.Event()
pause_event = threading.Event()

def gpu_idle_worker():
    print('[idle_worker] GPU idle worker started.')
    running = True
    while not stop_event.is_set():
        if pause_event.is_set():
            if running:
                running = False
            time.sleep(0.1)
            continue
        else:
            if not running:
                running = True
        try:
            a = torch.rand((2000, 2000), dtype=torch.float32, device='cuda')
            b = torch.rand((2000, 2000), dtype=torch.float32, device='cuda')
            torch.matmul(a, b)
            torch.cuda.synchronize()
        except RuntimeError:
            time.sleep(1)
    print('[idle_worker] GPU idle worker stopped.')

idle_thread = threading.Thread(target=gpu_idle_worker, daemon=True)
idle_thread.start()

# ---------------------------- Gmsh Execution ----------------------- #

def execute_gmsh_script(code: str, timeout: int = 120):
    """
    Execute Gmsh script in isolated environment and analyze results.

    Returns:
        {
            "status": "success" | "error",
            "mesh_generated": bool,
            "execution_time": float,
            "mesh_stats": {...},
            "stdout": str,
            "stderr": str
        }
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        try:
            script_path = os.path.join(tmpdir, "mesh_script.py")
            output_mesh = os.path.join(tmpdir, "output.msh")

            # Ensure script saves to our output path
            if "gmsh.write(" not in code:
                code = code.replace("gmsh.finalize()", f'gmsh.write("{output_mesh}")\ngmsh.finalize()')
            else:
                code = re.sub(r'gmsh\.write\(["\'].*?["\']\)', f'gmsh.write("{output_mesh}")', code)

            with open(script_path, 'w') as f:
                f.write(code)

            # Execute with timeout
            start_time = time.time()
            result = subprocess.run(
                ['python', script_path],
                cwd=tmpdir,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            execution_time = time.time() - start_time

            # Check if mesh was generated
            if os.path.exists(output_mesh):
                mesh_stats = analyze_mesh(output_mesh)
                return {
                    "status": "success",
                    "mesh_generated": True,
                    "execution_time": execution_time,
                    "mesh_stats": mesh_stats,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
            else:
                return {
                    "status": "error",
                    "mesh_generated": False,
                    "execution_time": execution_time,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "error": "No mesh file generated"
                }

        except subprocess.TimeoutExpired:
            return {
                "status": "error",
                "mesh_generated": False,
                "error": "Execution timeout"
            }
        except Exception as e:
            return {
                "status": "error",
                "mesh_generated": False,
                "error": str(e)
            }


def analyze_mesh(mesh_file: str):
    """
    Analyze generated mesh and extract quality metrics.

    Runs in a subprocess to avoid signal issues with gmsh in threaded Flask.
    """
    print(f"[analyze_mesh] Opening mesh file: {mesh_file}", flush=True)

    # Create a small script to analyze the mesh in a subprocess
    analysis_script = f'''
import json
import sys
import numpy as np

try:
    import gmsh
    gmsh.initialize()
    gmsh.option.setNumber("General.Terminal", 0)
    gmsh.open("{mesh_file}")

    stats = {{
        "num_nodes": 0,
        "num_elements": 0,
        "num_physical_groups": 0,
        "element_types": {{}},
        "quality_min": 0.0,
        "quality_mean": 0.0
    }}

    # Count nodes
    node_tags, _, _ = gmsh.model.mesh.getNodes()
    stats["num_nodes"] = len(node_tags)

    # Count elements
    elem_types = gmsh.model.mesh.getElementTypes()
    total_elements = 0
    for elem_type in elem_types:
        elem_tags, _ = gmsh.model.mesh.getElementsByType(elem_type)
        count = len(elem_tags)
        total_elements += count
        stats["element_types"][str(elem_type)] = count

    stats["num_elements"] = total_elements

    # Count physical groups
    for dim in range(4):
        groups = gmsh.model.getPhysicalGroups(dim)
        stats["num_physical_groups"] += len(groups)

    # Quality metrics (for tetrahedra, type 4)
    if 4 in elem_types:
        try:
            tet_tags, _ = gmsh.model.mesh.getElementsByType(4)
            if len(tet_tags) > 0:
                qualities = gmsh.model.mesh.getElementQualities(tet_tags, "minSICN")
                if len(qualities) > 0:
                    stats["quality_min"] = float(np.min(qualities))
                    stats["quality_mean"] = float(np.mean(qualities))
        except:
            pass

    gmsh.finalize()
    print(json.dumps(stats))
except Exception as e:
    print(json.dumps({{"num_nodes": 0, "num_elements": 0, "num_physical_groups": 0, "error": str(e)}}))
'''

# --- Subprocess-based Script Execution (isolated) ---
import subprocess

def execute_gmsh_script(script_code, timeout=30):
    """
    Execute a Gmsh script in subprocess for full isolation.
    Each script runs in its own process with its own gmsh instance.
    Returns tuple (success: bool, num_elements: int, error: str)
    """
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(script_code)
            temp_path = f.name

        # Run in subprocess - complete isolation
        result = subprocess.run(
            ['python', temp_path],
            capture_output=True,
            text=True,
            timeout=timeout
        )

        os.unlink(temp_path)

        # Check output for element count (gmsh prints "X nodes Y elements")
        output = result.stdout + result.stderr
        elem_match = re.search(r'(\d+)\s+nodes\s+(\d+)\s+elements', output)
        if elem_match:
            num_elements = int(elem_match.group(2))
            return (num_elements > 0, num_elements, None)

        # If no match but no error, check return code
        if result.returncode == 0:
            return (True, 1, None)  # Assume success

        return (False, 0, result.stderr[:200] if result.stderr else "Unknown error")

    except subprocess.TimeoutExpired:
        try:
            os.unlink(temp_path)
        except:
            pass
        return (False, 0, "timeout")
    except Exception as e:
        return (False, 0, str(e))


def extract_python_code(text):
    """Extract Python code from markdown code blocks."""
    pattern = r'```python\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return matches[-1].strip()
    # Try without language specifier
    pattern = r'```\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return matches[-1].strip()
    return None


# --- Main Script Logic ---

# 1. Load and Prepare Data
print(f"[{args.suffix}] Loading data from: {INPUT_FILE}")
try:
    with open(INPUT_FILE, "r") as f:
        data = json.load(f)
    # Clean up the input file immediately after loading
    os.remove(INPUT_FILE)
except FileNotFoundError:
    print(f"[{args.suffix}] ERROR: Input file not found. Exiting.")
    exit()

# Filter data - tasks with score >= 0 are valid (score = -1 means invalid format)
valid_data = [item for item in data if item.get('score', -1) >= 0 and item.get('task')]
if not valid_data:
    print(f"[{args.suffix}] No valid tasks to process. Exiting.")
    with open(OUTPUT_FILE, "w") as f:
        json.dump([], f)
    exit()

tasks = [item["task"] for item in valid_data]
print(f"[{args.suffix}] Found {len(tasks)} tasks to process.")

# 2. Initialize Model and Tokenizer
print(f"[{args.suffix}] Initializing vLLM for model: {args.model}")
tokenizer = AutoTokenizer.from_pretrained(args.model)
model = vllm.LLM(
    model=args.model,
    tokenizer=args.model,
    gpu_memory_utilization=0.85,
    seed=int(args.suffix),
)
sample_params = vllm.SamplingParams(
    max_tokens=4096,
    temperature=1.0,
    top_p=1.0,
    top_k=40,
    stop_token_ids=[tokenizer.eos_token_id],
    n=args.num_samples,
)

# 3. Generate Responses
print(f"[{args.suffix}] Generating {args.num_samples} samples for each task...")
chats = [[
    {"role": "system", "content": GMSH_SYSTEM_PROMPT},
    {"role": "user", "content": task}
] for task in tasks]

if tokenizer.chat_template:
    prompts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True, add_special_tokens=True) for chat in chats]
else:
    prompts = ["system: " + chat[0]["content"] + '\n' + "user: " + chat[1]["content"] for chat in chats]

responses = model.generate(prompts, sampling_params=sample_params, use_tqdm=True)
print(f"[{args.suffix}] Generation complete.")

# 4. Process and Validate Responses
results_all = []
print(f"[{args.suffix}] Validating Gmsh scripts...")
for response, task in zip(responses, tasks):
    try:
        # Extract Python code from all generated samples
        scripts = [extract_python_code(output.text) for output in response.outputs]
        scripts = [s for s in scripts if s]  # Filter out None/empty

        if not scripts:
            print(f"[{args.suffix}] WARNING: No valid scripts found for task: '{task[:50]}...'")
            continue

        # Validate each script
        validation_results = []
        for script in scripts:
            result = execute_gmsh_script(script, timeout=30)
            validation_results.append(result)

        # Count successful scripts
        successful = sum(1 for r in validation_results if r[0])
        total = len(validation_results)

        if successful == 0:
            print(f"[{args.suffix}] No working scripts for: '{task[:50]}...'")
            continue

        # Score based on success rate
        score = successful / total

        # Find the best script (most elements generated)
        best_idx = max(range(len(validation_results)),
                       key=lambda i: validation_results[i][1] if validation_results[i][0] else -1)
        best_script = scripts[best_idx]

        results_all.append({
            "question": task,  # Keep as 'question' for compatibility
            "answer": best_script,  # The best working script
            "score": score,
            "successful_scripts": successful,
            "total_scripts": total,
        })

    except Exception as e:
        print(f"[{args.suffix}] CRITICAL ERROR processing task '{task[:50]}...': {e}")
        continue

# 5. Save Final Results
print(f"[{args.suffix}] Processed {len(results_all)} tasks. Saving results to: {OUTPUT_FILE}")
with open(OUTPUT_FILE, "w") as f:
    json.dump(results_all, f, indent=4)

print(f"[{args.suffix}] Script finished.")
